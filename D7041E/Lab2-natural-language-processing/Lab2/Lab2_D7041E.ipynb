{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> D7041E Lab2</h1>\n",
    "Sune Larsson, sunlar-3@student.ltu.se\n",
    "\n",
    "Elliot Eriksson, lelrek-1@student.ltu.se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Word2Vec\n",
    "***\n",
    "\n",
    "The dimensions used for evaluating Word2Vec performance were **[10, 250, 1000]**. Each dimension was tested over **five simulations**, yielding the following average accuracy:\n",
    "\n",
    "- **Dimension: 10**, Average Accuracy: 49.50%, Average time: 54.68s\n",
    "- **Dimension: 250**, Average Accuracy: 69.75%, Average time: 109.73s\n",
    "- **Dimension: 1000**, Average Accuracy: 70.25%, Average time: 289.80s\n",
    "- (Threshold frequency = 0.00055 for all measurements)\n",
    "\n",
    "#### Analysis\n",
    "- **10 Dimensions**: The model's limited capacity at 10 dimensions restricts its ability to identify synonyms effectively. However, it still significantly outperforms random guessing (20-25% accuracy).\n",
    "\n",
    "- **250 Dimensions**: This configuration achieved the highest accuracy. It likely provides enough capacity to capture the relationships between words without introducing excessive noise.\n",
    "\n",
    "- **1000 Dimensions**: Although better than 10 dimensions, the accuracy drops slightly compared to 250 dimensions. This may be due to redundancies, increased noise, or overfitting.\n",
    "\n",
    "#### Conclusion\n",
    "The **optimal dimensionality** lies around **250**, offering the best trade-off between capacity and noise. Further exploration around this dimensionality could reveal even better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Indexing Word Embedding Analysis with TOEFL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Word Size Problem\n",
    "\n",
    "#### Problem\n",
    "The original code was designed to work correctly **only with a window size of 2** due to **hardcoded logic** for handling neighbors. This made the code inflexible and unsuitable for other window sizes.\n",
    "\n",
    "#### Issues\n",
    "1. **Hardcoded Neighbor Handling**:\n",
    "   - The code explicitly checks for neighbors up to **2 positions away**, making it impossible to generalize to different window sizes.\n",
    "\n",
    "2. **Edge Cases**:\n",
    "   - The code fails to handle cases where there are **fewer neighbors** than the window size, potentially causing errors or incorrect updates.\n",
    "\n",
    "#### Solution\n",
    "To generalize the code for any window size:\n",
    "1. **Iterate Over Neighbors**:\n",
    "   - Modify the logic to **iterate over neighbors** dynamically, based on the specified window size, both to the **left and right** of the focus word.\n",
    "\n",
    "2. **Handle Edge Cases**:\n",
    "   - Implement checks to ensure the code correctly handles **bounds** and uses **available neighbors**, even if there are fewer than the specified window size.\n",
    "\n",
    "By generalizing the neighbor handling logic, the code can now work for **any window size**, making it more flexible and ensuring correct updates based on the chosen context window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while line != \"\":\n",
    "            if line != \"\\n\":\n",
    "                lines.append(line.split())\n",
    "                words = lines[2]\n",
    "                length = len(words)\n",
    "                i = 0\n",
    "                while i < length:\n",
    "                    if not (word_space.get(words[i]) is None):\n",
    "                        k = 1\n",
    "                        word_space_vector = word_space[words[i]]\n",
    "                        while (i - k >= 0) and (k <= window_size): #process left neighbors of the focus word\n",
    "                            word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[words[i - k]], -1))         \n",
    "                            k += 1\n",
    "                        # Handle different situations if there was not enough neighbors on the left in the current line    \n",
    "                        if k <= window_size and (len(lines[1])>0): \n",
    "                            if len(lines[1]) < 2:\n",
    "                                if k != 1: #if one word on the left was already added\n",
    "                                    word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[1][0]], -1)) #update word embedding\n",
    "                                else:\n",
    "                                    word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                                np.roll(dictionary[lines[1][0]], -1)) #update word embedding\n",
    "                                    word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                                np.roll(dictionary[lines[0][len(lines[0]) - 1]], -1)) #update word embedding\n",
    "                            else:\n",
    "                                if k != 1:\n",
    "                                    word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                                np.roll(dictionary[lines[1][len(lines[1]) - 1]], -1)) #update word embedding\n",
    "                                else:\n",
    "                                    word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                                np.roll(dictionary[lines[1][len(lines[1]) - 1]], -1)) #update word embedding\n",
    "                                    word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                                np.roll(dictionary[lines[1][len(lines[1]) - 2]], -1)) #update word embedding\n",
    "\n",
    "                        k = 1\n",
    "                        while (i + k < length) and (k <= window_size): #process right neighbors of the focus word\n",
    "                            word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[words[i + k]], 1)) #update word embedding\n",
    "                            k += 1\n",
    "                        if k <= window_size:\n",
    "                            if len(lines[3]) < 2:\n",
    "                                if k != 1:\n",
    "                                    word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
    "                                else:\n",
    "                                    word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
    "                                    word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[4][0]], 1)) #update word embedding\n",
    "                            else:\n",
    "                                if k != 1:\n",
    "                                    word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
    "                                else:\n",
    "                                    word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
    "                                    word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                            np.roll(dictionary[lines[3][1]], 1))\n",
    "\n",
    "                    i += 1\n",
    "                lines.pop(0)\n",
    "            line = text_file.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generalizes the handling of neighbors by iterating up to the specified window size and checking for the presence of neighbors in the previous and next lines. This should allow the code to work for any window size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if line != \"\\n\":\n",
    "    lines.append(line.split())\n",
    "    words = lines[2]\n",
    "    length = len(words)\n",
    "    i = 0\n",
    "    while i < length:\n",
    "        if not (word_space.get(words[i]) is None):\n",
    "            k = 1\n",
    "            word_space_vector = word_space[words[i]]\n",
    "            # Process left neighbors of the focus word\n",
    "            while (i - k >= 0) and (k <= window_size):\n",
    "                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[words[i - k]], -1))\n",
    "                k += 1\n",
    "            # Handle different situations if there were not enough neighbors on the left in the current line\n",
    "            if k <= window_size:\n",
    "                for j in range(k, window_size + 1):\n",
    "                    if len(lines[1]) >= j:\n",
    "                        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[1][len(lines[1]) - j]], -1))\n",
    "                    elif len(lines[0]) > 0:\n",
    "                        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[0][len(lines[0]) - 1]], -1))\n",
    "\n",
    "            k = 1\n",
    "            # Process right neighbors of the focus word\n",
    "            while (i + k < length) and (k <= window_size):\n",
    "                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[words[i + k]], 1))\n",
    "                k += 1\n",
    "            if k <= window_size:\n",
    "                for j in range(k, window_size + 1):\n",
    "                    if len(lines[3]) >= j:\n",
    "                        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][j - 1]], 1))\n",
    "                    elif len(lines[4]) > 0:\n",
    "                        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[4][0]], 1))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of Random Indexing (RI)\n",
    "***\n",
    "\n",
    "The following are the results for **three different dimensionalities** with Random Indexing (RI). Each measurement includes **average accuracy** and **average time** with five simulation.\n",
    "\n",
    "- **1000 Dimensions**: **64.50%** accuracy, **34.96s** average time\n",
    "- **4000 Dimensions**: **61.75%** accuracy, **47.98s** average time\n",
    "- **10000 Dimensions**: **63.75%** accuracy, **82.81s** average time\n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "- **Accuracy vs. Dimensionality**:\n",
    "  - Accuracy **decreases** from **64.50%** (1000 dimensions) to **61.75%** (4000 dimensions). This drop suggests that the increased dimensionality might introduce noise or lead to overfitting.\n",
    "  - At **10000 dimensions**, accuracy **increases slightly** to **63.75%**, indicating that while more dimensions can improve representation, they may also reintroduce complexity and noise, affecting generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Conclusion: Computational Demand Comparison\n",
    "\n",
    "#### Accuracy Comparison\n",
    "By comparing both methods, **Word2Vec** achieved the **highest accuracy**, approximately **4%** better than **Random Indexing (RI)**.\n",
    "\n",
    "#### Best Performance Results\n",
    "The **best performance** for each method was achieved with the following settings:\n",
    "- **Word2Vec**: **250 Dimensions**, **Accuracy: 71.5%**, **Run Time: 109.73s** (5 runs)\n",
    "- **Random Indexing (RI)**: **4000 Dimensions**, **Accuracy: 67.5%**, **Run Time: 47.98s** (5 runs)\n",
    "\n",
    "#### Final Conclusion\n",
    "- If the primary objective is to achieve the **highest accuracy**, and **computational power is not a constraint**, then **Word2Vec** is the better option.\n",
    "- Conversely, if **computational efficiency** and **limited resources** are the key factors, **Random Indexing (RI)** is preferable, as it achieves reasonable accuracy with much lower computational demand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
