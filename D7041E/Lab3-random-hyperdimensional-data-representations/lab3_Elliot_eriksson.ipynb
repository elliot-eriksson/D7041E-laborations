{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets and preproccesing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data():\n",
    "# # Folder containing the training data\n",
    "#     folder_path = 'training-data'\n",
    "\n",
    "#     train_data = {}\n",
    "#     train_data['sentence'] = []\n",
    "#     train_data['language'] = []\n",
    "#     # List to store dataframes\n",
    "#     i = 0\n",
    "#     # Loop through each file in the folder\n",
    "#     for folderName in os.listdir(folder_path):\n",
    "#         folderPathName = os.path.join(folder_path, folderName)\n",
    "#         if os.path.isdir(os.path.join(folder_path, folderName)):\n",
    "#             code = folderName.split('_')[0][:3]\n",
    "#             train_data['language'].append(code)\n",
    "#         for filename in  os.listdir(folderPathName):\n",
    "#             if filename.endswith('sentences.txt'): \n",
    "#                 i +=1\n",
    "#                 file_path = os.path.join(folderPathName, filename)\n",
    "#                 # Read the data from the text file\n",
    "#                 with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#                     for line in file.readlines():\n",
    "#                         if line.strip() != '':\n",
    "#                             train_data['sentence'].append(line.strip())\n",
    "\n",
    "#     return train_data\n",
    "# train_data = load_data()\n",
    "# print(train_data['sentence'][5])\n",
    "# array =np.array(train_data)\n",
    "# print(len(train_data['sentence']))\n",
    "\n",
    "def load_data():\n",
    "    folder_path = 'training-data'\n",
    "\n",
    "    train_data = {'sentence': [], 'language': []}\n",
    "    # Loop through each file in the folder\n",
    "    for folderName in os.listdir(folder_path):\n",
    "        folderPathName = os.path.join(folder_path, folderName)\n",
    "        if os.path.isdir(folderPathName):\n",
    "            code = folderName.split('_')[0][:3]\n",
    "            for filename in os.listdir(folderPathName):\n",
    "                if filename.endswith('sentences.txt'): \n",
    "                    file_path = os.path.join(folderPathName, filename)\n",
    "                    # Read the data from the text file\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                        for line in file.readlines():\n",
    "                            if line.strip() != '':\n",
    "                                train_data['sentence'].append(line.strip())\n",
    "                                train_data['language'].append(code)\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data():\n",
    "# # Folder containing the training data\n",
    "#     folder_path = 'training-data'\n",
    "\n",
    "#     # List to store dataframes\n",
    "#     dataframes = []\n",
    "#     language_codes = []\n",
    "#     i = 0\n",
    "#     # Loop through each file in the folder\n",
    "#     for folderName in os.listdir(folder_path):\n",
    "#         folderPathName = os.path.join(folder_path, folderName)\n",
    "#         if os.path.isdir(os.path.join(folder_path, folderName)):\n",
    "#             code = folderName.split('_')[0][:3]\n",
    "#             language_codes.append(code)\n",
    "#         for filename in  os.listdir(folderPathName):\n",
    "#             if filename.endswith('sentences.txt'): \n",
    "#                 i +=1\n",
    "#                 file_path = os.path.join(folderPathName, filename)\n",
    "#                 # Read the data from the text file\n",
    "#                 df = open(file_path, 'r', encoding='utf-8').read()\n",
    "#                 dataframes.append(df)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     return np.array(dataframes), language_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing training data\n",
    "1. **Lower casing and removing accents**\n",
    "2. **Removing puncatation and special characters** `., etc`\n",
    "3. **tokenazation using `Tri-grams`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_non_words(text):\n",
    "    return ' '.join(re.findall(r'\\b[a-zA-Z]+\\b', text))\n",
    "\n",
    "def clean_whitespace(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def latin_unicode(text):\n",
    "    return unidecode(text)\n",
    "\n",
    "def preprocess_sentence(train_data):\n",
    "    for (i, sentence) in enumerate(train_data['sentence']):\n",
    "        train_data['sentence'][i] = latin_unicode(sentence)\n",
    "        train_data['sentence'][i] = lower_case(train_data['sentence'][i])\n",
    "        train_data['sentence'][i] = remove_non_words(train_data['sentence'][i])\n",
    "        train_data['sentence'][i] = clean_whitespace(train_data['sentence'][i])\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def preprocess_data(dataframes):\n",
    "    # traindataframe = {}\n",
    "    # testdataframe = {}\n",
    "    # validationdataframe = {}\n",
    "\n",
    "    traindataframe = []\n",
    "    testdataframe = []\n",
    "    validationdataframe = []\n",
    "    for i in range(len(dataframes)):\n",
    "        dataframes[i] = latin_unicode(dataframes[i])\n",
    "        dataframes[i] = lower_case(dataframes[i])\n",
    "        dataframes[i] = remove_non_words(dataframes[i])\n",
    "        dataframes[i] = clean_whitespace(dataframes[i])\n",
    "        # print(dataframes[i][5:10])\n",
    "        traindataframe.append(dataframes[i][:6000])\n",
    "        testdataframe.append(dataframes[i][6000:8000])\n",
    "        validationdataframe.append(dataframes[i][8000:10000])\n",
    "    # return dataframes, traindataframe, testdataframe, validationdataframe\n",
    "    return dataframes, np.array(traindataframe), np.array(testdataframe), np.array(validationdataframe)\n",
    "    # return dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding of n-grams using d-dimension {+1,-1} distributed representation\n",
    "- Use of n = 3 `tri grams`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text]./h-calculation.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 100\n",
    "alphabet = ' abcdefghijklmnopqrstuvwxyz'\n",
    "# item_memory = np.random.choice([-1, 1], size=(len(alphabet), d))\n",
    "n_gram_size = 3\n",
    "item_memory = np.random.choice([-1, 1], size=(len(alphabet), d))\n",
    "permutations = [np.random.choice([-1, 1], size=d) for _ in range(n_gram_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n=3):\n",
    "    ngrams = [text[i:i+n_gram_size] for i in range(len(text) - n_gram_size + 1)]\n",
    "    return ngrams\n",
    "\n",
    "def encode_ngram(ngram):\n",
    "        encoded_vector = np.ones(d)\n",
    "\n",
    "        for j, char in enumerate(ngram):\n",
    "            if char == ' ':\n",
    "                index = 0  \n",
    "            elif char.isalpha():\n",
    "                index = ord(char.lower()) - ord('a') + 1  \n",
    "            else:\n",
    "                continue  \n",
    "\n",
    "            # Handle out-of-bounds index (if any)\n",
    "            if 0 <= index < len(alphabet):\n",
    "                permuted_vector = item_memory[index] * permutations[j]\n",
    "                encoded_vector *= permuted_vector\n",
    "        return encoded_vector    \n",
    "\n",
    "\n",
    "def calculate_vector(texts, d):\n",
    "# Initialize item memory and permutations\n",
    "    ngrams = generate_ngrams(texts)\n",
    "    bundled_vector = np.zeros(d)\n",
    "    for ngram in ngrams:\n",
    "        ngram_vector = encode_ngram(ngram)\n",
    "        bundled_vector += ngram_vector\n",
    "    norm = np.linalg.norm(bundled_vector)\n",
    "    return bundled_vector / norm if norm != 0 else bundled_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conventional Local Representation Size\n",
    "The size of a vector in a conventional (local) representation depends on the number of possible n-grams in the given alphabet. The formula for this is \\( S = a^n \\), where \\( a \\) is the size of the alphabet and \\( n \\) is the order of the n-gram.\n",
    "\n",
    "For example, if we are using the Latin alphabet (26 letters) and \\( n = 3 \\) (trigrams), the size of the vector is:\n",
    "\n",
    "\\[\n",
    "S = 27^3 = 19,683\n",
    "\\]\n",
    "\n",
    "### Difficulties of Working with Conventional Representations of n-Grams in Machine Learning\n",
    "As mentioned earlier, the size of the one-hot encoded vector for trigrams in the Latin alphabet is \\( 19,683 \\). This high dimensionality poses an obvious computational bottleneck due to the rapid exponential growth of the number of possible n-grams as \\( n \\) increases.\n",
    "\n",
    "Additionally, these vectors are typically sparse, meaning most entries are zero. This sparsity can lead to inefficiencies in both memory usage and processing time. Furthermore, the large vector size increases the risk of overfitting, as models may learn to memorize the training data instead of generalizing to new, unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using hyperdimensional centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data\n",
    "\n",
    "In our test the sentences in the test data is at least 61 symbols long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\boink\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from unidecode import unidecode\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "def preprocess_test_data(text):\n",
    "    # Step 1: Remove metadata tags like <SPEAKER ...> and <P>\n",
    "    text_no_tags = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    text = latin_unicode(text_no_tags)\n",
    "    text = lower_case(text)\n",
    "    text = sent_tokenize(text)\n",
    "\n",
    "    text_pre = []\n",
    "    for sentence in text:\n",
    "        sentence = remove_non_words(sentence)\n",
    "        sentence = clean_whitespace(sentence)\n",
    "        text_pre.append(sentence)\n",
    "\n",
    "    return text_pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def extract_text_from_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_specific_test_data(test_dir, languages):\n",
    "    # List to store dataframes\n",
    "    lines_per_language = 100\n",
    "    languages_codes_2digits = {'bul': 'bg', 'ces': 'cs', 'dan': 'da', 'deu': 'de', 'ell': 'el', 'eng': 'en', 'est': 'et', 'fin': 'fi', 'fra': 'fr', 'hun': 'hu', 'ita': 'it', 'lav': 'lv', 'lit': 'lt', 'nld': 'nl', 'pol': 'pl', 'por': 'pt', 'ron': 'ro', 'slk': 'sk', 'slv': 'sl', 'spa': 'es', 'swe': 'sv'}\n",
    "    test_data = []\n",
    "    # Loop through each file in the folder\n",
    "    for lang_idx, lang in enumerate(languages, start=1):\n",
    "        language_dir = os.path.join(test_dir, languages_codes_2digits[lang])\n",
    "        file_pattern = os.path.join(language_dir, \"ep-*.txt\")\n",
    "        file_paths = glob.glob(file_pattern)\n",
    "\n",
    "        total_rows = 0\n",
    "        language_data = []\n",
    "        for file_idx, file_path in enumerate(file_paths, start=1):\n",
    "            text = extract_text_from_file(file_path)\n",
    "            if text:\n",
    "                newtext = preprocess_test_data(text)\n",
    "                newtext = [item for item in newtext if len(item) > 61]\n",
    "                if total_rows + len(newtext) > lines_per_language:\n",
    "                    newtext = newtext[:lines_per_language - total_rows]\n",
    "                    language_data = language_data + newtext\n",
    "                    break\n",
    "                language_data = language_data + newtext\n",
    "                total_rows += len(newtext)\n",
    "        if len(language_data) != lines_per_language:\n",
    "            print(f\"Warning: Language {lang} has {len(language_data)} lines instead of 1000\")\n",
    "            if len(language_data) == 1:\n",
    "                print(language_data)\n",
    "        test_data.append({'text': language_data, 'language': lang})\n",
    "    \n",
    "    return pd.DataFrame(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "def similarity_measure(X, V):\n",
    "    # Calculate the dot product X Â· V\n",
    "    dot_product = np.dot(X, V)\n",
    "    \n",
    "    norm_X = np.linalg.norm(X)\n",
    "    norm_V = np.linalg.norm(V)\n",
    "\n",
    "    similarity = dot_product / (norm_X * norm_V)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def classify_language(test_sentence, languages,centroids, d):\n",
    "    test_vector = calculate_vector(test_sentence, d)\n",
    "\n",
    "    similarities = [similarity_measure(test_vector, centroid) for centroid in centroids]\n",
    "\n",
    "    return languages[np.argmax(similarities)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_accuracy(predicted_labels_all, test_labels, language_codes):\n",
    "    test_labels = np.repeat(test_labels, -(-len(predicted_labels_all) // len(test_labels)))\n",
    "    test_labels = test_labels[:len(predicted_labels_all)]  \n",
    "\n",
    "    cm = confusion_matrix(test_labels, predicted_labels_all, labels=language_codes)\n",
    "    accuracy = accuracy_score(test_labels, predicted_labels_all)\n",
    "    f1 = f1_score(test_labels, predicted_labels_all, average='weighted')\n",
    "\n",
    "    # Print results\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    print(f'F1 Score: {f1*100:.2f}%')\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def confusion_matrix_heatmap(cm, language_codes):\n",
    "    # Create a confusion matrix heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=language_codes,\n",
    "                yticklabels=language_codes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653018\n",
      "2000\n",
      "2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 102\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(split_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup3\u001b[39m\u001b[38;5;124m'\u001b[39m]))  \u001b[38;5;66;03m# Third group of English sentences\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# traindataframe = dataframe_pre[:6000]\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# testdataframe = dataframe_pre[6000:8000]\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# validationdataframe = dataframe_pre[8000:10000]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# print(\"dataframe\", dataframe_pre[20])\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# print(\"traindata\", traindataframe)\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m test_data2 \u001b[38;5;241m=\u001b[39m \u001b[43mload_specific_test_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest-data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage_codes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m test_data_pre2 \u001b[38;5;241m=\u001b[39m test_data2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    104\u001b[0m test_data_pre2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(test_data_pre2)\n",
      "Cell \u001b[1;32mIn[68], line 24\u001b[0m, in \u001b[0;36mload_specific_test_data\u001b[1;34m(test_dir, languages)\u001b[0m\n\u001b[0;32m     22\u001b[0m language_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_idx, file_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(file_paths, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 24\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m text:\n\u001b[0;32m     26\u001b[0m         newtext \u001b[38;5;241m=\u001b[39m preprocess_test_data(text)\n",
      "Cell \u001b[1;32mIn[68], line 4\u001b[0m, in \u001b[0;36mextract_text_from_file\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text_from_file\u001b[39m(file_path):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\boink\\Desktop\\Kod\\D7041E-laborations\\D7041E\\Lab3-random-hyperdimensional-data-representations\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen codecs>:309\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from typing import OrderedDict\n",
    "\n",
    "\n",
    "def test(dataframe_pre, language_codes, test_data_pre, d):\n",
    "    language_centroids = OrderedDict((lang, np.zeros(d)) for lang in language_codes)\n",
    "    \n",
    "    for text, lang in zip(dataframe_pre, language_codes):\n",
    "        # language_centroids[lang] += normalize_vector(calculate_vector(text, d))\n",
    "        language_centroids[lang] += calculate_vector(text, d)\n",
    "    \n",
    "    # print(\"language_centroids \", language_centroids)\n",
    "    centroids = np.array(list(language_centroids.values()))\n",
    "    languages = list(language_centroids.keys())\n",
    "    predicted_labels = np.array([classify_language(text,languages,centroids, d) for text in test_data_pre])\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "# dataframes, language_codes = load_data()\n",
    "testload = load_data()\n",
    "# dataframes = testload['sentence']\n",
    "language_codes = testload['language']\n",
    "# dataframe_pre = preprocess_data(dataframes)\n",
    "dataframe_pre = preprocess_sentence(testload)\n",
    "\n",
    "# print('dataframe ', dataframe_pre['sentence'][200901])\n",
    "\n",
    "# traindata = []\n",
    "\n",
    "# i = 0\n",
    "# for text, languages in zip(dataframe_pre['sentence'], dataframe_pre['language']):\n",
    "    \n",
    "#     if i < 10:\n",
    "#         print('language', languages, \"text\", text, )\n",
    "#         joined_text = ' '.join(text)\n",
    "\n",
    "#         i += 1\n",
    "#     # for sentence in text:\n",
    "#     #     traindata.append(sentence)\n",
    "#     #     i += 1\n",
    "#     #     if i == 100:\n",
    "#     #         break\n",
    "\n",
    "# print(dataframe_pre.keys())\n",
    "\n",
    "# for language in dataframe_pre['language']:\n",
    "#     print('language', language)\n",
    "#     for sentences in dataframe_pre['sentence'] if dataframe_pre['language'] == language:\n",
    "#         print('sentence', sentences)\n",
    "#         # traindata.append(sentences)\n",
    "#         break \n",
    "\n",
    "\n",
    "def split_sentences_by_language(train_data):\n",
    "    split_data = {}\n",
    "\n",
    "    # Get a list of unique languages\n",
    "    unique_languages = set(dataframe_pre['language'])\n",
    "\n",
    "    for language in unique_languages:\n",
    "        # Get the sentences for the current language\n",
    "        language_sentences = [dataframe_pre['sentence'][i] for i in range(len(dataframe_pre['sentence'])) if dataframe_pre['language'][i] == language]\n",
    "\n",
    "        # Ensure we have 10000 sentences for each language\n",
    "        if len(language_sentences) == 10000:\n",
    "            # Split the sentences into 3 groups: 6000, 2000, 2000\n",
    "            group1 = language_sentences[:6000]\n",
    "            group2 = language_sentences[6000:8000]\n",
    "            group3 = language_sentences[8000:]\n",
    "\n",
    "            group1_text = ' '.join(group1)\n",
    "\n",
    "            # Store the groups for this language\n",
    "            split_data[language] = {\n",
    "                \"group1\": group1_text,\n",
    "                \"group2\": group2,\n",
    "                \"group3\": group3\n",
    "            }\n",
    "\n",
    "    return split_data\n",
    "split_data = split_sentences_by_language(dataframe_pre)\n",
    "print(len(split_data['eng']['group1']))  # First group of English sentences\n",
    "print(len(split_data['eng']['group2']))  # Second group of English sentences\n",
    "print(len(split_data['eng']['group3']))  # Third group of English sentences\n",
    "\n",
    "# traindataframe = dataframe_pre[:6000]\n",
    "# testdataframe = dataframe_pre[6000:8000]\n",
    "# validationdataframe = dataframe_pre[8000:10000]\n",
    "\n",
    "# for i in range(len(dataframe_pre)):\n",
    "#     traindataframe[i] = dataframe_pre[i][:6000]\n",
    "# traindataframetest = ' '.join(traindataframe)\n",
    "\n",
    "# print(\"traindataframe\", traindataframetest[0])\n",
    "# print(\"testdataframe\", traindataframe)\n",
    "\n",
    "\n",
    "# dataframe_pre, traindataframe, testdataframe, validationdataframe = preprocess_data(dataframes)\n",
    "# print(\"test data\", traindataframe[20], testdataframe[20], validationdataframe[20])\n",
    "# print(\"dataframe\", dataframe_pre[20])\n",
    "# print(\"traindata\", traindataframe)\n",
    "\n",
    "test_data2 = load_specific_test_data('test-data', language_codes)\n",
    "test_data_pre2 = test_data2['text']\n",
    "test_data_pre2 = np.array(test_data_pre2)\n",
    "test_labels = [text[-3:] for text in test_data2['language']]\n",
    "test_data_pre2 = np.concatenate(test_data_pre2).ravel()\n",
    "\n",
    "predicted_labels_all = test(traindataframe, language_codes, testdataframe, 100)\n",
    "\n",
    "cm = prediction_accuracy(predicted_labels_all, test_labels, language_codes)\n",
    "confusion_matrix_heatmap(cm, language_codes)\n",
    "\n",
    "# # predicted_labels_all2 = test(traindataframe, language_codes, test_data_pre2, 1000)\n",
    "# cm = prediction_accuracy(predicted_labels_all2, test_labels, language_codes)\n",
    "# confusion_matrix_heatmap(cm, language_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trigram with words: Accuracy: 4.38 %\n",
    "F1 Score: 1.57 %\n",
    "trigram with letters:\n",
    "Accuracy: 5.14 %\n",
    "F1 Score: 2.06 %"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
