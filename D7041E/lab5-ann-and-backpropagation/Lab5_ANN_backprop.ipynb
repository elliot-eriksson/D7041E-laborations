{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n",
    "\n",
    "def f_relu(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return np.maximum(0, X)\n",
    "    else:\n",
    "        return (X > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_with_err(err_str):\n",
    "    print >> sys.stderr, err_str\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100, activation=f_sigmoid):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         activation=activation))\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=f_softmax))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        # exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "        # Computes the error of the output (i.e loss function)\n",
    "\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "            # exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            # calculate the delta values (error signals) for each layer, excluding the bias terms.\n",
    "            \n",
    "            \n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                # exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "                #The gradient descent algorithm is used to update the weights via backpropagation\n",
    "                # by moving them in the direction that minimizes the loss function.\n",
    "                #eta is the learning rate, Hyperparameter that controls step size in updating weights\n",
    "\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data, labels):\n",
    "    yhat = model.forward_propagate(data)\n",
    "    yhat = np.argmax(yhat, axis=1)\n",
    "    return np.sum(yhat == labels) / float(labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "Xtr = Xtr.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "Xtr = Xtr.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "Xtr /= 255\n",
    "X_test /= 255\n",
    "print(Xtr.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.57643 Test error: 0.57780\n",
      "[   1]  Training error: 0.07788 Test error: 0.07740\n",
      "[   2]  Training error: 0.05045 Test error: 0.05600\n",
      "[   3]  Training error: 0.03900 Test error: 0.04280\n",
      "[   4]  Training error: 0.03260 Test error: 0.03670\n",
      "[   5]  Training error: 0.02582 Test error: 0.03560\n",
      "[   6]  Training error: 0.02465 Test error: 0.03570\n",
      "[   7]  Training error: 0.02427 Test error: 0.03570\n",
      "[   8]  Training error: 0.02155 Test error: 0.03480\n",
      "[   9]  Training error: 0.01913 Test error: 0.03400\n",
      "[  10]  Training error: 0.01875 Test error: 0.03440\n",
      "[  11]  Training error: 0.01422 Test error: 0.02970\n",
      "[  12]  Training error: 0.02162 Test error: 0.03620\n",
      "[  13]  Training error: 0.01637 Test error: 0.03300\n",
      "[  14]  Training error: 0.01638 Test error: 0.03420\n",
      "[  15]  Training error: 0.01237 Test error: 0.03020\n",
      "[  16]  Training error: 0.01475 Test error: 0.03390\n",
      "[  17]  Training error: 0.00998 Test error: 0.03150\n",
      "[  18]  Training error: 0.00790 Test error: 0.03030\n",
      "[  19]  Training error: 0.00865 Test error: 0.03090\n",
      "[  20]  Training error: 0.00933 Test error: 0.03110\n",
      "[  21]  Training error: 0.01122 Test error: 0.03340\n",
      "[  22]  Training error: 0.01053 Test error: 0.03350\n",
      "[  23]  Training error: 0.00773 Test error: 0.03000\n",
      "[  24]  Training error: 0.01722 Test error: 0.03840\n",
      "[  25]  Training error: 0.00815 Test error: 0.03130\n",
      "[  26]  Training error: 0.00963 Test error: 0.03170\n",
      "[  27]  Training error: 0.00717 Test error: 0.03140\n",
      "[  28]  Training error: 0.00960 Test error: 0.03020\n",
      "[  29]  Training error: 0.00627 Test error: 0.03120\n",
      "[  30]  Training error: 0.00450 Test error: 0.02960\n",
      "[  31]  Training error: 0.01522 Test error: 0.03580\n",
      "[  32]  Training error: 0.00795 Test error: 0.03250\n",
      "[  33]  Training error: 0.00293 Test error: 0.02910\n",
      "[  34]  Training error: 0.00860 Test error: 0.03370\n",
      "[  35]  Training error: 0.00252 Test error: 0.02960\n",
      "[  36]  Training error: 0.00152 Test error: 0.02800\n",
      "[  37]  Training error: 0.00080 Test error: 0.02780\n",
      "[  38]  Training error: 0.00052 Test error: 0.02720\n",
      "[  39]  Training error: 0.00030 Test error: 0.02710\n",
      "[  40]  Training error: 0.00023 Test error: 0.02640\n",
      "[  41]  Training error: 0.00017 Test error: 0.02670\n",
      "[  42]  Training error: 0.00010 Test error: 0.02650\n",
      "[  43]  Training error: 0.00007 Test error: 0.02680\n",
      "[  44]  Training error: 0.00005 Test error: 0.02680\n",
      "[  45]  Training error: 0.00003 Test error: 0.02740\n",
      "[  46]  Training error: 0.00003 Test error: 0.02680\n",
      "[  47]  Training error: 0.00003 Test error: 0.02700\n",
      "[  48]  Training error: 0.00002 Test error: 0.02670\n",
      "[  49]  Training error: 0.00002 Test error: 0.02680\n",
      "[  50]  Training error: 0.00002 Test error: 0.02650\n",
      "[  51]  Training error: 0.00002 Test error: 0.02660\n",
      "[  52]  Training error: 0.00002 Test error: 0.02670\n",
      "[  53]  Training error: 0.00002 Test error: 0.02660\n",
      "[  54]  Training error: 0.00002 Test error: 0.02660\n",
      "[  55]  Training error: 0.00002 Test error: 0.02660\n",
      "[  56]  Training error: 0.00002 Test error: 0.02640\n",
      "[  57]  Training error: 0.00002 Test error: 0.02640\n",
      "[  58]  Training error: 0.00002 Test error: 0.02640\n",
      "[  59]  Training error: 0.00002 Test error: 0.02630\n",
      "[  60]  Training error: 0.00002 Test error: 0.02620\n",
      "[  61]  Training error: 0.00002 Test error: 0.02610\n",
      "[  62]  Training error: 0.00002 Test error: 0.02610\n",
      "[  63]  Training error: 0.00002 Test error: 0.02600\n",
      "[  64]  Training error: 0.00002 Test error: 0.02600\n",
      "[  65]  Training error: 0.00002 Test error: 0.02600\n",
      "[  66]  Training error: 0.00000 Test error: 0.02600\n",
      "[  67]  Training error: 0.00000 Test error: 0.02600\n",
      "[  68]  Training error: 0.00000 Test error: 0.02610\n",
      "[  69]  Training error: 0.00000 Test error: 0.02610\n",
      "Accuracy on test data: 97.39%\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "eta = 0.05 #Default learning rate\n",
    "print(f'learning rate {eta} are used for training')\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels, eval_train=True, eta=eta)\n",
    "\n",
    "accuarcy = get_accuracy(mlp, X_test, L_test)\n",
    "print(\"Accuracy on test data: {0:.2f}%\".format(accuarcy*100))\n",
    "\n",
    "print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  differences in the functionality of the multi-layer perceptron\n",
    "1. **Learning rate** = `0.005` \n",
    "2. **Learning rate** = `0.5` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 epochs and learning rate 0.005 are used for training\n",
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.70335 Test error: 0.70070\n",
      "[   1]  Training error: 0.64715 Test error: 0.64320\n",
      "[   2]  Training error: 0.59943 Test error: 0.59790\n",
      "[   3]  Training error: 0.45213 Test error: 0.46330\n",
      "[   4]  Training error: 0.21243 Test error: 0.20410\n",
      "[   5]  Training error: 0.11463 Test error: 0.11340\n",
      "[   6]  Training error: 0.08957 Test error: 0.08910\n",
      "[   7]  Training error: 0.07602 Test error: 0.07660\n",
      "[   8]  Training error: 0.06415 Test error: 0.06490\n",
      "[   9]  Training error: 0.05600 Test error: 0.05680\n",
      "[  10]  Training error: 0.04903 Test error: 0.05170\n",
      "[  11]  Training error: 0.04390 Test error: 0.04650\n",
      "[  12]  Training error: 0.03888 Test error: 0.04290\n",
      "[  13]  Training error: 0.03482 Test error: 0.03880\n",
      "[  14]  Training error: 0.03172 Test error: 0.03590\n",
      "[  15]  Training error: 0.02895 Test error: 0.03440\n",
      "[  16]  Training error: 0.02702 Test error: 0.03240\n",
      "[  17]  Training error: 0.02498 Test error: 0.03110\n",
      "[  18]  Training error: 0.02330 Test error: 0.03080\n",
      "[  19]  Training error: 0.02162 Test error: 0.03030\n",
      "[  20]  Training error: 0.01992 Test error: 0.02980\n",
      "[  21]  Training error: 0.01848 Test error: 0.02900\n",
      "[  22]  Training error: 0.01715 Test error: 0.02810\n",
      "[  23]  Training error: 0.01595 Test error: 0.02800\n",
      "[  24]  Training error: 0.01492 Test error: 0.02700\n",
      "[  25]  Training error: 0.01383 Test error: 0.02650\n",
      "[  26]  Training error: 0.01277 Test error: 0.02590\n",
      "[  27]  Training error: 0.01165 Test error: 0.02640\n",
      "[  28]  Training error: 0.01078 Test error: 0.02600\n",
      "[  29]  Training error: 0.01010 Test error: 0.02570\n",
      "[  30]  Training error: 0.00955 Test error: 0.02560\n",
      "[  31]  Training error: 0.00895 Test error: 0.02560\n",
      "[  32]  Training error: 0.00820 Test error: 0.02520\n",
      "[  33]  Training error: 0.00762 Test error: 0.02500\n",
      "[  34]  Training error: 0.00725 Test error: 0.02510\n",
      "[  35]  Training error: 0.00670 Test error: 0.02500\n",
      "[  36]  Training error: 0.00622 Test error: 0.02500\n",
      "[  37]  Training error: 0.00585 Test error: 0.02490\n",
      "[  38]  Training error: 0.00540 Test error: 0.02500\n",
      "[  39]  Training error: 0.00503 Test error: 0.02520\n",
      "[  40]  Training error: 0.00453 Test error: 0.02500\n",
      "[  41]  Training error: 0.00415 Test error: 0.02510\n",
      "[  42]  Training error: 0.00387 Test error: 0.02490\n",
      "[  43]  Training error: 0.00348 Test error: 0.02480\n",
      "[  44]  Training error: 0.00322 Test error: 0.02460\n",
      "[  45]  Training error: 0.00270 Test error: 0.02440\n",
      "[  46]  Training error: 0.00242 Test error: 0.02420\n",
      "[  47]  Training error: 0.00230 Test error: 0.02390\n",
      "[  48]  Training error: 0.00212 Test error: 0.02380\n",
      "[  49]  Training error: 0.00198 Test error: 0.02380\n",
      "[  50]  Training error: 0.00192 Test error: 0.02390\n",
      "[  51]  Training error: 0.00168 Test error: 0.02360\n",
      "[  52]  Training error: 0.00150 Test error: 0.02320\n",
      "[  53]  Training error: 0.00135 Test error: 0.02330\n",
      "[  54]  Training error: 0.00125 Test error: 0.02340\n",
      "[  55]  Training error: 0.00115 Test error: 0.02330\n",
      "[  56]  Training error: 0.00107 Test error: 0.02340\n",
      "[  57]  Training error: 0.00102 Test error: 0.02340\n",
      "[  58]  Training error: 0.00098 Test error: 0.02310\n",
      "[  59]  Training error: 0.00083 Test error: 0.02320\n",
      "[  60]  Training error: 0.00078 Test error: 0.02290\n",
      "[  61]  Training error: 0.00068 Test error: 0.02280\n",
      "[  62]  Training error: 0.00063 Test error: 0.02290\n",
      "[  63]  Training error: 0.00060 Test error: 0.02320\n",
      "[  64]  Training error: 0.00057 Test error: 0.02300\n",
      "[  65]  Training error: 0.00055 Test error: 0.02330\n",
      "[  66]  Training error: 0.00050 Test error: 0.02330\n",
      "[  67]  Training error: 0.00047 Test error: 0.02330\n",
      "[  68]  Training error: 0.00045 Test error: 0.02320\n",
      "[  69]  Training error: 0.00042 Test error: 0.02310\n",
      "Accuracy on test data: 97.69%\n"
     ]
    }
   ],
   "source": [
    "num_epochs=70\n",
    "eta=0.005\n",
    "print(f'{num_epochs} epochs and learning rate {eta} are used for training')\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,num_epochs=num_epochs,eta=eta ,eval_train=True)\n",
    "\n",
    "accuarcy = get_accuracy(mlp, X_test, L_test)\n",
    "print(\"Accuracy on test data: {0:.2f}%\".format(accuarcy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 epochs and learning rate 0.5 are used for training\n",
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.90248 Test error: 0.90260\n",
      "[   1]  Training error: 0.88763 Test error: 0.88650\n",
      "[   2]  Training error: 0.90137 Test error: 0.90420\n",
      "[   3]  Training error: 0.90085 Test error: 0.89910\n",
      "[   4]  Training error: 0.90128 Test error: 0.90200\n",
      "[   5]  Training error: 0.90263 Test error: 0.90180\n",
      "[   6]  Training error: 0.90085 Test error: 0.89910\n",
      "[   7]  Training error: 0.90137 Test error: 0.90420\n",
      "[   8]  Training error: 0.88763 Test error: 0.88650\n",
      "[   9]  Training error: 0.88763 Test error: 0.88650\n",
      "[  10]  Training error: 0.90137 Test error: 0.90420\n",
      "[  11]  Training error: 0.90248 Test error: 0.90260\n",
      "[  12]  Training error: 0.89782 Test error: 0.89900\n",
      "[  13]  Training error: 0.90085 Test error: 0.89910\n",
      "[  14]  Training error: 0.90070 Test error: 0.89680\n",
      "[  15]  Training error: 0.89782 Test error: 0.89900\n",
      "[  16]  Training error: 0.90137 Test error: 0.90420\n",
      "[  17]  Training error: 0.90263 Test error: 0.90180\n",
      "[  18]  Training error: 0.90248 Test error: 0.90260\n",
      "[  19]  Training error: 0.90248 Test error: 0.90260\n",
      "[  20]  Training error: 0.89782 Test error: 0.89900\n",
      "[  21]  Training error: 0.90248 Test error: 0.90260\n",
      "[  22]  Training error: 0.90965 Test error: 0.91080\n",
      "[  23]  Training error: 0.90248 Test error: 0.90260\n",
      "[  24]  Training error: 0.89558 Test error: 0.89720\n",
      "[  25]  Training error: 0.90128 Test error: 0.90200\n",
      "[  26]  Training error: 0.90085 Test error: 0.89910\n",
      "[  27]  Training error: 0.90137 Test error: 0.90420\n",
      "[  28]  Training error: 0.90248 Test error: 0.90260\n",
      "[  29]  Training error: 0.90965 Test error: 0.91080\n",
      "[  30]  Training error: 0.90137 Test error: 0.90420\n",
      "[  31]  Training error: 0.89782 Test error: 0.89900\n",
      "[  32]  Training error: 0.89782 Test error: 0.89900\n",
      "[  33]  Training error: 0.90085 Test error: 0.89910\n",
      "[  34]  Training error: 0.90128 Test error: 0.90200\n",
      "[  35]  Training error: 0.90965 Test error: 0.91080\n",
      "[  36]  Training error: 0.88763 Test error: 0.88650\n",
      "[  37]  Training error: 0.89782 Test error: 0.89900\n",
      "[  38]  Training error: 0.90137 Test error: 0.90420\n",
      "[  39]  Training error: 0.90137 Test error: 0.90420\n",
      "[  40]  Training error: 0.88763 Test error: 0.88650\n",
      "[  41]  Training error: 0.90085 Test error: 0.89910\n",
      "[  42]  Training error: 0.90263 Test error: 0.90180\n",
      "[  43]  Training error: 0.90128 Test error: 0.90200\n",
      "[  44]  Training error: 0.89782 Test error: 0.89900\n",
      "[  45]  Training error: 0.90965 Test error: 0.91080\n",
      "[  46]  Training error: 0.90070 Test error: 0.89680\n",
      "[  47]  Training error: 0.90128 Test error: 0.90200\n",
      "[  48]  Training error: 0.90248 Test error: 0.90260\n",
      "[  49]  Training error: 0.90248 Test error: 0.90260\n",
      "[  50]  Training error: 0.90137 Test error: 0.90420\n",
      "[  51]  Training error: 0.90137 Test error: 0.90420\n",
      "[  52]  Training error: 0.90248 Test error: 0.90260\n",
      "[  53]  Training error: 0.88763 Test error: 0.88650\n",
      "[  54]  Training error: 0.88763 Test error: 0.88650\n",
      "[  55]  Training error: 0.90070 Test error: 0.89680\n",
      "[  56]  Training error: 0.90085 Test error: 0.89910\n",
      "[  57]  Training error: 0.89558 Test error: 0.89720\n",
      "[  58]  Training error: 0.90248 Test error: 0.90260\n",
      "[  59]  Training error: 0.90248 Test error: 0.90260\n",
      "[  60]  Training error: 0.90248 Test error: 0.90260\n",
      "[  61]  Training error: 0.89782 Test error: 0.89900\n",
      "[  62]  Training error: 0.90263 Test error: 0.90180\n",
      "[  63]  Training error: 0.90248 Test error: 0.90260\n",
      "[  64]  Training error: 0.90248 Test error: 0.90260\n",
      "[  65]  Training error: 0.89782 Test error: 0.89900\n",
      "[  66]  Training error: 0.90070 Test error: 0.89680\n",
      "[  67]  Training error: 0.88763 Test error: 0.88650\n",
      "[  68]  Training error: 0.90085 Test error: 0.89910\n",
      "[  69]  Training error: 0.89782 Test error: 0.89900\n",
      "Accuracy on test data: 10.10%\n"
     ]
    }
   ],
   "source": [
    "num_epochs=70\n",
    "eta=0.5\n",
    "print(f'{num_epochs} epochs and learning rate {eta} are used for training')\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,num_epochs=num_epochs,eta=eta ,eval_train=True)\n",
    "\n",
    "accuarcy = get_accuracy(mlp, X_test, L_test)\n",
    "print(\"Accuracy on test data: {0:.2f}%\".format(accuarcy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.89558 Test error: 0.89720\n",
      "[   1]  Training error: 0.89558 Test error: 0.89720\n",
      "[   2]  Training error: 0.89558 Test error: 0.89720\n",
      "[   3]  Training error: 0.89558 Test error: 0.89720\n",
      "[   4]  Training error: 0.89558 Test error: 0.89720\n",
      "[   5]  Training error: 0.89558 Test error: 0.89720\n",
      "[   6]  Training error: 0.89558 Test error: 0.89720\n",
      "[   7]  Training error: 0.89558 Test error: 0.89720\n",
      "[   8]  Training error: 0.89558 Test error: 0.89720\n",
      "[   9]  Training error: 0.89558 Test error: 0.89720\n",
      "[  10]  Training error: 0.89558 Test error: 0.89720\n",
      "[  11]  Training error: 0.89558 Test error: 0.89720\n",
      "[  12]  Training error: 0.89558 Test error: 0.89720\n",
      "[  13]  Training error: 0.89558 Test error: 0.89720\n",
      "[  14]  Training error: 0.89558 Test error: 0.89720\n",
      "[  15]  Training error: 0.89558 Test error: 0.89720\n",
      "[  16]  Training error: 0.89558 Test error: 0.89720\n",
      "[  17]  Training error: 0.89558 Test error: 0.89720\n",
      "[  18]  Training error: 0.89558 Test error: 0.89720\n",
      "[  19]  Training error: 0.89558 Test error: 0.89720\n",
      "[  20]  Training error: 0.89558 Test error: 0.89720\n",
      "[  21]  Training error: 0.88210 Test error: 0.88280\n",
      "[  22]  Training error: 0.38098 Test error: 0.38730\n",
      "[  23]  Training error: 0.07587 Test error: 0.07660\n",
      "[  24]  Training error: 0.04580 Test error: 0.05050\n",
      "[  25]  Training error: 0.03880 Test error: 0.04500\n",
      "[  26]  Training error: 0.03403 Test error: 0.04240\n",
      "[  27]  Training error: 0.03072 Test error: 0.03930\n",
      "[  28]  Training error: 0.02782 Test error: 0.03710\n",
      "[  29]  Training error: 0.02538 Test error: 0.03630\n",
      "[  30]  Training error: 0.02762 Test error: 0.03860\n",
      "[  31]  Training error: 0.02860 Test error: 0.04190\n",
      "[  32]  Training error: 0.01933 Test error: 0.03370\n",
      "[  33]  Training error: 0.02060 Test error: 0.03280\n",
      "[  34]  Training error: 0.01473 Test error: 0.03130\n",
      "[  35]  Training error: 0.02018 Test error: 0.03630\n",
      "[  36]  Training error: 0.01498 Test error: 0.03330\n",
      "[  37]  Training error: 0.01555 Test error: 0.03260\n",
      "[  38]  Training error: 0.01173 Test error: 0.03070\n",
      "[  39]  Training error: 0.01082 Test error: 0.03240\n",
      "[  40]  Training error: 0.01203 Test error: 0.03280\n",
      "[  41]  Training error: 0.01420 Test error: 0.03510\n",
      "[  42]  Training error: 0.01012 Test error: 0.03060\n",
      "[  43]  Training error: 0.01535 Test error: 0.03630\n",
      "[  44]  Training error: 0.01217 Test error: 0.03150\n",
      "[  45]  Training error: 0.00755 Test error: 0.02940\n",
      "[  46]  Training error: 0.00937 Test error: 0.03170\n",
      "[  47]  Training error: 0.01003 Test error: 0.02930\n",
      "[  48]  Training error: 0.00777 Test error: 0.03110\n",
      "[  49]  Training error: 0.00922 Test error: 0.03140\n",
      "[  50]  Training error: 0.00543 Test error: 0.02800\n",
      "[  51]  Training error: 0.00523 Test error: 0.02820\n",
      "[  52]  Training error: 0.00830 Test error: 0.03380\n",
      "[  53]  Training error: 0.00790 Test error: 0.03150\n",
      "[  54]  Training error: 0.00660 Test error: 0.02890\n",
      "[  55]  Training error: 0.00632 Test error: 0.02770\n",
      "[  56]  Training error: 0.00658 Test error: 0.02960\n",
      "[  57]  Training error: 0.00530 Test error: 0.02780\n",
      "[  58]  Training error: 0.00610 Test error: 0.02900\n",
      "[  59]  Training error: 0.00442 Test error: 0.02960\n",
      "[  60]  Training error: 0.00317 Test error: 0.02650\n",
      "[  61]  Training error: 0.00393 Test error: 0.02670\n",
      "[  62]  Training error: 0.00183 Test error: 0.02630\n",
      "[  63]  Training error: 0.00287 Test error: 0.02750\n",
      "[  64]  Training error: 0.00647 Test error: 0.02750\n",
      "[  65]  Training error: 0.00653 Test error: 0.03000\n",
      "[  66]  Training error: 0.00397 Test error: 0.02870\n",
      "[  67]  Training error: 0.00318 Test error: 0.02760\n",
      "[  68]  Training error: 0.00472 Test error: 0.03010\n",
      "[  69]  Training error: 0.00303 Test error: 0.02510\n",
      "Accuracy on test data: 97.49%\n"
     ]
    }
   ],
   "source": [
    "eta = 0.005\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size, activation=f_relu)\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,eval_train=True, eta=eta)\n",
    "\n",
    "accuarcy = get_accuracy(mlp, X_test, L_test)\n",
    "print(\"Accuracy on test data: {0:.2f}%\".format(accuarcy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't know why this is needed\n",
    "# batch_size=100;\n",
    "\n",
    "\n",
    "# train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels)\n",
    "\n",
    "# mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "# mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "#              eval_train=True)\n",
    "\n",
    "# print(\"Done:)\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions 1\n",
    "\n",
    "### A: Core Concept of the Backpropagation Algorithm\n",
    "Backpropagation is an optimization technique used to train neural networks. It works by calculating the difference between the predicted output and the actual result, and then adjusting the network's weights by propagating this error back through its layers.\n",
    "\n",
    "### B: Purpose and Function of the Softmax Layer\n",
    "The Softmax function transforms a set of raw prediction scores into probabilities. This normalization ensures that the sum of all outputs equals 1, allowing the outputs to represent the likelihood of each class.\n",
    "\n",
    "### C: Frequently Used Non-Linear Output Functions and Their Effects\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit):**\n",
    "- **Key Attribute:** Highly efficient in terms of computation.\n",
    "- **Application:** Widely used in hidden layers to address issues like vanishing gradients.\n",
    "\n",
    "2. **Sigmoid Function:**\n",
    "- **Key Attribute:** Produces values between 0 and 1, making it ideal for representing probabilities.\n",
    "- **Application:** Commonly used in binary classification problems.\n",
    "\n",
    "3. **Softmax Function:**\n",
    "- **Key Attribute:** Similar to Sigmoid, it outputs probabilities but does so across multiple classes, ensuring the total is always 1.\n",
    "- **Application:** Typically used in the output layer of multi-class classification models to determine the predicted class.\n",
    "\n",
    "4. **Hyperbolic Tangent (Tanh):**\n",
    "- **Key Attribute:** Outputs values ranging from -1 to 1, making it effective for differentiating between negative, neutral, and positive states.\n",
    "- **Application:** Often utilized as an activation function in hidden layers because it is zero-centered, reducing systematic biases in the model.\n",
    "- **Benefit:** Enables faster and more effective gradient descent by ensuring a balanced distribution of gradients during training.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
