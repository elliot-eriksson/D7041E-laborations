{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n",
    "\n",
    "def f_relu(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return np.maximum(0, X)\n",
    "    else:\n",
    "        return (X > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_with_err(err_str):\n",
    "    print >> sys.stderr, err_str\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100, activation=f_sigmoid):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         activation=activation))\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=f_softmax))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        # exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "        # Computes the error of the output (i.e loss function)\n",
    "\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "            # exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            # calculate the delta values (error signals) for each layer, excluding the bias terms.\n",
    "            \n",
    "            \n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                # exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "                #The gradient descent algorithm is used to update the weights via backpropagation\n",
    "                # by moving them in the direction that minimizes the loss function.\n",
    "                #eta is the learning rate, Hyperparameter that controls step size in updating weights\n",
    "\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "Xtr = Xtr.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "Xtr = Xtr.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "Xtr /= 255\n",
    "X_test /= 255\n",
    "print(Xtr.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.34562 Test error: 0.34500\n",
      "[   1]  Training error: 0.11352 Test error: 0.11130\n",
      "[   2]  Training error: 0.04923 Test error: 0.05470\n",
      "[   3]  Training error: 0.03882 Test error: 0.04620\n",
      "[   4]  Training error: 0.03475 Test error: 0.04270\n",
      "[   5]  Training error: 0.02910 Test error: 0.03880\n",
      "[   6]  Training error: 0.02893 Test error: 0.04040\n",
      "[   7]  Training error: 0.02547 Test error: 0.03810\n",
      "[   8]  Training error: 0.02185 Test error: 0.03570\n",
      "[   9]  Training error: 0.02207 Test error: 0.03540\n",
      "[  10]  Training error: 0.01755 Test error: 0.03200\n",
      "[  11]  Training error: 0.01728 Test error: 0.03180\n",
      "[  12]  Training error: 0.01660 Test error: 0.03350\n",
      "[  13]  Training error: 0.01907 Test error: 0.03440\n",
      "[  14]  Training error: 0.01597 Test error: 0.03310\n",
      "[  15]  Training error: 0.01350 Test error: 0.03250\n",
      "[  16]  Training error: 0.01253 Test error: 0.03180\n",
      "[  17]  Training error: 0.01035 Test error: 0.03070\n",
      "[  18]  Training error: 0.01037 Test error: 0.03110\n",
      "[  19]  Training error: 0.01017 Test error: 0.03050\n",
      "[  20]  Training error: 0.01253 Test error: 0.03270\n",
      "[  21]  Training error: 0.01515 Test error: 0.03550\n",
      "[  22]  Training error: 0.01220 Test error: 0.03340\n",
      "[  23]  Training error: 0.00758 Test error: 0.02970\n",
      "[  24]  Training error: 0.00967 Test error: 0.02970\n",
      "[  25]  Training error: 0.00888 Test error: 0.03110\n",
      "[  26]  Training error: 0.00837 Test error: 0.03040\n",
      "[  27]  Training error: 0.00542 Test error: 0.02850\n",
      "[  28]  Training error: 0.00628 Test error: 0.03010\n",
      "[  29]  Training error: 0.00607 Test error: 0.02950\n",
      "[  30]  Training error: 0.00618 Test error: 0.02790\n",
      "[  31]  Training error: 0.00450 Test error: 0.02860\n",
      "[  32]  Training error: 0.01243 Test error: 0.03400\n",
      "[  33]  Training error: 0.00393 Test error: 0.02850\n",
      "[  34]  Training error: 0.00535 Test error: 0.02970\n",
      "[  35]  Training error: 0.00950 Test error: 0.03490\n",
      "[  36]  Training error: 0.01982 Test error: 0.04150\n",
      "[  37]  Training error: 0.01520 Test error: 0.03850\n",
      "[  38]  Training error: 0.01045 Test error: 0.03380\n",
      "[  39]  Training error: 0.00482 Test error: 0.02800\n",
      "[  40]  Training error: 0.00445 Test error: 0.02800\n",
      "[  41]  Training error: 0.00683 Test error: 0.03260\n",
      "[  42]  Training error: 0.00208 Test error: 0.02840\n",
      "[  43]  Training error: 0.00522 Test error: 0.02930\n",
      "[  44]  Training error: 0.00195 Test error: 0.02820\n",
      "[  45]  Training error: 0.00197 Test error: 0.02880\n",
      "[  46]  Training error: 0.00187 Test error: 0.02840\n",
      "[  47]  Training error: 0.00048 Test error: 0.02750\n",
      "[  48]  Training error: 0.00023 Test error: 0.02720\n",
      "[  49]  Training error: 0.00035 Test error: 0.02800\n",
      "[  50]  Training error: 0.00010 Test error: 0.02730\n",
      "[  51]  Training error: 0.00012 Test error: 0.02720\n",
      "[  52]  Training error: 0.00005 Test error: 0.02700\n",
      "[  53]  Training error: 0.00013 Test error: 0.02720\n",
      "[  54]  Training error: 0.00003 Test error: 0.02700\n",
      "[  55]  Training error: 0.00003 Test error: 0.02720\n",
      "[  56]  Training error: 0.00012 Test error: 0.02740\n",
      "[  57]  Training error: 0.00003 Test error: 0.02720\n",
      "[  58]  Training error: 0.00008 Test error: 0.02730\n",
      "[  59]  Training error: 0.00003 Test error: 0.02740\n",
      "[  60]  Training error: 0.00002 Test error: 0.02670\n",
      "[  61]  Training error: 0.00002 Test error: 0.02710\n",
      "[  62]  Training error: 0.00002 Test error: 0.02650\n",
      "[  63]  Training error: 0.00002 Test error: 0.02680\n",
      "[  64]  Training error: 0.00002 Test error: 0.02660\n",
      "[  65]  Training error: 0.00002 Test error: 0.02660\n",
      "[  66]  Training error: 0.00002 Test error: 0.02670\n",
      "[  67]  Training error: 0.00002 Test error: 0.02680\n",
      "[  68]  Training error: 0.00002 Test error: 0.02680\n",
      "[  69]  Training error: 0.00002 Test error: 0.02690\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels, eval_train=True,)\n",
    "\n",
    "print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 epochs and learning rate 0.05 are used for training\n",
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.35753 Test error: 0.36340\n",
      "[   1]  Training error: 0.11043 Test error: 0.10760\n",
      "[   2]  Training error: 0.05450 Test error: 0.06000\n",
      "[   3]  Training error: 0.04525 Test error: 0.05090\n",
      "[   4]  Training error: 0.03265 Test error: 0.04220\n",
      "[   5]  Training error: 0.03487 Test error: 0.04490\n",
      "[   6]  Training error: 0.04260 Test error: 0.05500\n",
      "[   7]  Training error: 0.02938 Test error: 0.04180\n",
      "[   8]  Training error: 0.02920 Test error: 0.04140\n",
      "[   9]  Training error: 0.03528 Test error: 0.05070\n",
      "[  10]  Training error: 0.02622 Test error: 0.03920\n",
      "[  11]  Training error: 0.02573 Test error: 0.04010\n",
      "[  12]  Training error: 0.02067 Test error: 0.03700\n",
      "[  13]  Training error: 0.02272 Test error: 0.03840\n",
      "[  14]  Training error: 0.02572 Test error: 0.04000\n",
      "[  15]  Training error: 0.01707 Test error: 0.03410\n",
      "[  16]  Training error: 0.01493 Test error: 0.03260\n",
      "[  17]  Training error: 0.01333 Test error: 0.03340\n",
      "[  18]  Training error: 0.01347 Test error: 0.03160\n",
      "[  19]  Training error: 0.01392 Test error: 0.03370\n",
      "[  20]  Training error: 0.01058 Test error: 0.02920\n",
      "[  21]  Training error: 0.01237 Test error: 0.03530\n",
      "[  22]  Training error: 0.01037 Test error: 0.03190\n",
      "[  23]  Training error: 0.01165 Test error: 0.03330\n",
      "[  24]  Training error: 0.00845 Test error: 0.03060\n",
      "[  25]  Training error: 0.01275 Test error: 0.03410\n",
      "[  26]  Training error: 0.00823 Test error: 0.03050\n",
      "[  27]  Training error: 0.01290 Test error: 0.03330\n",
      "[  28]  Training error: 0.00750 Test error: 0.03030\n",
      "[  29]  Training error: 0.01490 Test error: 0.03670\n",
      "[  30]  Training error: 0.00720 Test error: 0.03080\n",
      "[  31]  Training error: 0.01115 Test error: 0.03360\n",
      "[  32]  Training error: 0.00803 Test error: 0.03300\n",
      "[  33]  Training error: 0.00622 Test error: 0.02970\n",
      "[  34]  Training error: 0.00602 Test error: 0.02870\n",
      "[  35]  Training error: 0.00395 Test error: 0.02730\n",
      "[  36]  Training error: 0.01155 Test error: 0.03460\n",
      "[  37]  Training error: 0.01345 Test error: 0.03370\n",
      "[  38]  Training error: 0.00782 Test error: 0.03020\n",
      "[  39]  Training error: 0.01110 Test error: 0.03240\n",
      "[  40]  Training error: 0.01023 Test error: 0.03240\n",
      "[  41]  Training error: 0.00730 Test error: 0.03130\n",
      "[  42]  Training error: 0.00275 Test error: 0.02780\n",
      "[  43]  Training error: 0.00340 Test error: 0.02990\n",
      "[  44]  Training error: 0.00263 Test error: 0.02840\n",
      "[  45]  Training error: 0.00262 Test error: 0.02900\n",
      "[  46]  Training error: 0.00280 Test error: 0.02840\n",
      "[  47]  Training error: 0.00145 Test error: 0.02790\n",
      "[  48]  Training error: 0.00270 Test error: 0.02980\n",
      "[  49]  Training error: 0.00078 Test error: 0.02820\n",
      "[  50]  Training error: 0.00048 Test error: 0.02680\n",
      "[  51]  Training error: 0.00030 Test error: 0.02640\n",
      "[  52]  Training error: 0.00017 Test error: 0.02650\n",
      "[  53]  Training error: 0.00012 Test error: 0.02630\n",
      "[  54]  Training error: 0.00013 Test error: 0.02620\n",
      "[  55]  Training error: 0.00012 Test error: 0.02590\n",
      "[  56]  Training error: 0.00010 Test error: 0.02640\n",
      "[  57]  Training error: 0.00008 Test error: 0.02570\n",
      "[  58]  Training error: 0.00005 Test error: 0.02610\n",
      "[  59]  Training error: 0.00007 Test error: 0.02570\n",
      "[  60]  Training error: 0.00005 Test error: 0.02640\n",
      "[  61]  Training error: 0.00005 Test error: 0.02570\n",
      "[  62]  Training error: 0.00003 Test error: 0.02610\n",
      "[  63]  Training error: 0.00002 Test error: 0.02580\n",
      "[  64]  Training error: 0.00002 Test error: 0.02600\n",
      "[  65]  Training error: 0.00002 Test error: 0.02580\n",
      "[  66]  Training error: 0.00000 Test error: 0.02590\n",
      "[  67]  Training error: 0.00000 Test error: 0.02580\n",
      "[  68]  Training error: 0.00000 Test error: 0.02600\n",
      "[  69]  Training error: 0.00000 Test error: 0.02550\n"
     ]
    }
   ],
   "source": [
    "num_epochs=70\n",
    "eta=0.05\n",
    "print(f'{num_epochs} epochs and learning rate {eta} are used for training')\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,num_epochs=num_epochs,eta=eta ,eval_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  differences in the functionality of the multi-layer perceptron\n",
    "1. **Learning rate** = `0.005` \n",
    "2. **Learning rate** = `0.5` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 epochs and learning rate 0.005 are used for training\n",
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.70337 Test error: 0.70040\n",
      "[   1]  Training error: 0.64705 Test error: 0.64330\n",
      "[   2]  Training error: 0.59938 Test error: 0.59800\n",
      "[   3]  Training error: 0.45552 Test error: 0.46600\n",
      "[   4]  Training error: 0.21537 Test error: 0.20800\n",
      "[   5]  Training error: 0.11580 Test error: 0.11250\n",
      "[   6]  Training error: 0.09148 Test error: 0.09000\n",
      "[   7]  Training error: 0.07712 Test error: 0.07470\n",
      "[   8]  Training error: 0.06585 Test error: 0.06340\n",
      "[   9]  Training error: 0.05588 Test error: 0.05540\n",
      "[  10]  Training error: 0.04872 Test error: 0.04890\n",
      "[  11]  Training error: 0.04260 Test error: 0.04340\n",
      "[  12]  Training error: 0.03772 Test error: 0.04040\n",
      "[  13]  Training error: 0.03373 Test error: 0.03890\n",
      "[  14]  Training error: 0.03008 Test error: 0.03620\n",
      "[  15]  Training error: 0.02723 Test error: 0.03340\n",
      "[  16]  Training error: 0.02485 Test error: 0.03220\n",
      "[  17]  Training error: 0.02283 Test error: 0.02990\n",
      "[  18]  Training error: 0.02068 Test error: 0.02940\n",
      "[  19]  Training error: 0.01898 Test error: 0.02920\n",
      "[  20]  Training error: 0.01793 Test error: 0.02770\n",
      "[  21]  Training error: 0.01633 Test error: 0.02730\n",
      "[  22]  Training error: 0.01527 Test error: 0.02730\n",
      "[  23]  Training error: 0.01413 Test error: 0.02740\n",
      "[  24]  Training error: 0.01315 Test error: 0.02660\n",
      "[  25]  Training error: 0.01213 Test error: 0.02590\n",
      "[  26]  Training error: 0.01147 Test error: 0.02570\n",
      "[  27]  Training error: 0.01075 Test error: 0.02530\n",
      "[  28]  Training error: 0.01002 Test error: 0.02530\n",
      "[  29]  Training error: 0.00928 Test error: 0.02490\n",
      "[  30]  Training error: 0.00878 Test error: 0.02470\n",
      "[  31]  Training error: 0.00813 Test error: 0.02400\n",
      "[  32]  Training error: 0.00772 Test error: 0.02410\n",
      "[  33]  Training error: 0.00723 Test error: 0.02370\n",
      "[  34]  Training error: 0.00678 Test error: 0.02390\n",
      "[  35]  Training error: 0.00633 Test error: 0.02430\n",
      "[  36]  Training error: 0.00582 Test error: 0.02420\n",
      "[  37]  Training error: 0.00537 Test error: 0.02400\n",
      "[  38]  Training error: 0.00505 Test error: 0.02420\n",
      "[  39]  Training error: 0.00453 Test error: 0.02380\n",
      "[  40]  Training error: 0.00415 Test error: 0.02330\n",
      "[  41]  Training error: 0.00385 Test error: 0.02290\n",
      "[  42]  Training error: 0.00358 Test error: 0.02310\n",
      "[  43]  Training error: 0.00335 Test error: 0.02300\n",
      "[  44]  Training error: 0.00297 Test error: 0.02280\n",
      "[  45]  Training error: 0.00278 Test error: 0.02250\n",
      "[  46]  Training error: 0.00255 Test error: 0.02220\n",
      "[  47]  Training error: 0.00240 Test error: 0.02220\n",
      "[  48]  Training error: 0.00212 Test error: 0.02230\n",
      "[  49]  Training error: 0.00207 Test error: 0.02210\n",
      "[  50]  Training error: 0.00187 Test error: 0.02220\n",
      "[  51]  Training error: 0.00172 Test error: 0.02220\n",
      "[  52]  Training error: 0.00158 Test error: 0.02240\n",
      "[  53]  Training error: 0.00147 Test error: 0.02230\n",
      "[  54]  Training error: 0.00138 Test error: 0.02230\n",
      "[  55]  Training error: 0.00122 Test error: 0.02240\n",
      "[  56]  Training error: 0.00107 Test error: 0.02220\n",
      "[  57]  Training error: 0.00105 Test error: 0.02230\n",
      "[  58]  Training error: 0.00098 Test error: 0.02250\n",
      "[  59]  Training error: 0.00087 Test error: 0.02270\n",
      "[  60]  Training error: 0.00083 Test error: 0.02280\n",
      "[  61]  Training error: 0.00080 Test error: 0.02290\n",
      "[  62]  Training error: 0.00077 Test error: 0.02280\n",
      "[  63]  Training error: 0.00068 Test error: 0.02290\n",
      "[  64]  Training error: 0.00065 Test error: 0.02320\n",
      "[  65]  Training error: 0.00062 Test error: 0.02320\n",
      "[  66]  Training error: 0.00060 Test error: 0.02310\n",
      "[  67]  Training error: 0.00057 Test error: 0.02300\n",
      "[  68]  Training error: 0.00053 Test error: 0.02290\n",
      "[  69]  Training error: 0.00052 Test error: 0.02320\n"
     ]
    }
   ],
   "source": [
    "num_epochs=70\n",
    "eta=0.005\n",
    "print(f'{num_epochs} epochs and learning rate {eta} are used for training')\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,num_epochs=num_epochs,eta=eta ,eval_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 epochs and learning rate 0.5 are used for training\n",
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.90070 Test error: 0.89680\n",
      "[   1]  Training error: 0.90085 Test error: 0.89910\n",
      "[   2]  Training error: 0.90128 Test error: 0.90200\n",
      "[   3]  Training error: 0.90263 Test error: 0.90180\n",
      "[   4]  Training error: 0.88763 Test error: 0.88650\n",
      "[   5]  Training error: 0.90085 Test error: 0.89910\n",
      "[   6]  Training error: 0.90070 Test error: 0.89680\n",
      "[   7]  Training error: 0.90248 Test error: 0.90260\n",
      "[   8]  Training error: 0.90248 Test error: 0.90260\n",
      "[   9]  Training error: 0.90248 Test error: 0.90260\n",
      "[  10]  Training error: 0.89558 Test error: 0.89720\n",
      "[  11]  Training error: 0.90137 Test error: 0.90420\n",
      "[  12]  Training error: 0.90128 Test error: 0.90200\n",
      "[  13]  Training error: 0.90085 Test error: 0.89910\n",
      "[  14]  Training error: 0.90128 Test error: 0.90200\n",
      "[  15]  Training error: 0.90965 Test error: 0.91080\n",
      "[  16]  Training error: 0.90248 Test error: 0.90260\n",
      "[  17]  Training error: 0.89782 Test error: 0.89900\n",
      "[  18]  Training error: 0.88763 Test error: 0.88650\n",
      "[  19]  Training error: 0.89782 Test error: 0.89900\n",
      "[  20]  Training error: 0.89782 Test error: 0.89900\n",
      "[  21]  Training error: 0.89558 Test error: 0.89720\n",
      "[  22]  Training error: 0.90248 Test error: 0.90260\n",
      "[  23]  Training error: 0.90128 Test error: 0.90200\n",
      "[  24]  Training error: 0.90248 Test error: 0.90260\n",
      "[  25]  Training error: 0.90128 Test error: 0.90200\n",
      "[  26]  Training error: 0.88763 Test error: 0.88650\n",
      "[  27]  Training error: 0.90248 Test error: 0.90260\n",
      "[  28]  Training error: 0.90263 Test error: 0.90180\n",
      "[  29]  Training error: 0.89558 Test error: 0.89720\n",
      "[  30]  Training error: 0.90263 Test error: 0.90180\n",
      "[  31]  Training error: 0.89782 Test error: 0.89900\n",
      "[  32]  Training error: 0.90248 Test error: 0.90260\n",
      "[  33]  Training error: 0.90128 Test error: 0.90200\n",
      "[  34]  Training error: 0.90128 Test error: 0.90200\n",
      "[  35]  Training error: 0.90965 Test error: 0.91080\n",
      "[  36]  Training error: 0.90085 Test error: 0.89910\n",
      "[  37]  Training error: 0.89558 Test error: 0.89720\n",
      "[  38]  Training error: 0.90263 Test error: 0.90180\n",
      "[  39]  Training error: 0.90128 Test error: 0.90200\n",
      "[  40]  Training error: 0.90137 Test error: 0.90420\n",
      "[  41]  Training error: 0.89782 Test error: 0.89900\n",
      "[  42]  Training error: 0.88763 Test error: 0.88650\n",
      "[  43]  Training error: 0.90248 Test error: 0.90260\n",
      "[  44]  Training error: 0.90965 Test error: 0.91080\n",
      "[  45]  Training error: 0.88763 Test error: 0.88650\n",
      "[  46]  Training error: 0.90128 Test error: 0.90200\n",
      "[  47]  Training error: 0.89782 Test error: 0.89900\n",
      "[  48]  Training error: 0.90137 Test error: 0.90420\n",
      "[  49]  Training error: 0.88763 Test error: 0.88650\n",
      "[  50]  Training error: 0.90070 Test error: 0.89680\n",
      "[  51]  Training error: 0.89782 Test error: 0.89900\n",
      "[  52]  Training error: 0.90137 Test error: 0.90420\n",
      "[  53]  Training error: 0.90263 Test error: 0.90180\n",
      "[  54]  Training error: 0.89782 Test error: 0.89900\n",
      "[  55]  Training error: 0.90128 Test error: 0.90200\n",
      "[  56]  Training error: 0.90070 Test error: 0.89680\n",
      "[  57]  Training error: 0.90248 Test error: 0.90260\n",
      "[  58]  Training error: 0.90085 Test error: 0.89910\n",
      "[  59]  Training error: 0.90263 Test error: 0.90180\n",
      "[  60]  Training error: 0.90137 Test error: 0.90420\n",
      "[  61]  Training error: 0.90128 Test error: 0.90200\n",
      "[  62]  Training error: 0.90085 Test error: 0.89910\n",
      "[  63]  Training error: 0.90137 Test error: 0.90420\n",
      "[  64]  Training error: 0.90128 Test error: 0.90200\n",
      "[  65]  Training error: 0.89782 Test error: 0.89900\n",
      "[  66]  Training error: 0.90137 Test error: 0.90420\n",
      "[  67]  Training error: 0.90965 Test error: 0.91080\n",
      "[  68]  Training error: 0.90128 Test error: 0.90200\n",
      "[  69]  Training error: 0.90137 Test error: 0.90420\n"
     ]
    }
   ],
   "source": [
    "num_epochs=70\n",
    "eta=0.5\n",
    "print(f'{num_epochs} epochs and learning rate {eta} are used for training')\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,num_epochs=num_epochs,eta=eta ,eval_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.89558 Test error: 0.89720\n",
      "[   1]  Training error: 0.89558 Test error: 0.89720\n",
      "[   2]  Training error: 0.89558 Test error: 0.89720\n",
      "[   3]  Training error: 0.89558 Test error: 0.89720\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m train_data, train_labels, valid_data, valid_labels\u001b[38;5;241m=\u001b[39mprepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n\u001b[0;32m      3\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MultiLayerPerceptron(layer_config\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m10\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mbatch_size, activation\u001b[38;5;241m=\u001b[39mf_relu)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43meval_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[46], line 83\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.evaluate\u001b[1;34m(self, train_data, train_labels, test_data, test_labels, num_epochs, eta, eval_train, eval_test)\u001b[0m\n\u001b[0;32m     81\u001b[0m errs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b_data, b_labels \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_data, train_labels):\n\u001b[1;32m---> 83\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_propagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     yhat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     85\u001b[0m     errs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mb_labels[np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(b_labels)), yhat])\n",
      "Cell \u001b[1;32mIn[46], line 35\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.forward_propagate\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mZ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(data, np\u001b[38;5;241m.\u001b[39mones((data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_propagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mforward_propagate()\n",
      "Cell \u001b[1;32mIn[45], line 35\u001b[0m, in \u001b[0;36mLayer.forward_propagate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input:\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mZ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_output:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eta = 0.005\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size, activation=f_relu)\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,eval_train=True, eta=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Train_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m;\n\u001b[1;32m----> 3\u001b[0m train_data, train_labels, valid_data, valid_labels\u001b[38;5;241m=\u001b[39mprepare_for_backprop(batch_size, \u001b[43mTrain_images\u001b[49m, Train_labels, Valid_images, Valid_labels)\n\u001b[0;32m      5\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MultiLayerPerceptron(layer_config\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m10\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m      7\u001b[0m mlp\u001b[38;5;241m.\u001b[39mevaluate(train_data, train_labels, valid_data, valid_labels,\n\u001b[0;32m      8\u001b[0m              eval_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Train_images' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
